{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Validation Code Pipeline\n",
    "# This script is used to perform a CURIAL validation on a given clinical dataset by producing model predictions on a dataset, and comparing against a ground truth.\n",
    "# By using the LFD flag, can assess on a subset of patients with LFD results\n",
    "# The Admissions flag allows subsetting admitted patients from all-attenders (where available)\n",
    "# The LFDComposite flag considers either a positive LFD or a positive CURIAL result as a suspected-COVID-19 case\n",
    "# The LFDComposite flag requires the LFD flag to be be set to true as must subset patients to cohort who received an LFD test\n",
    "\n",
    "# Feature sets correspond to CURIAL version (OLO+Vitals (-Rapide), Bloods+Vitals (-Lab), Bloods+Gas+Vital (-1.0, as per derivation paper))\n",
    "\n",
    "# AS version 0.1 21 Apr 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from scipy.special import ndtri\n",
    "from math import sqrt\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset being evaluated (OHU / UHB / PUH / BH)\n",
    "dataset = pd.read_csv('OUH Wave 2 Attendances with LFTs.csv', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model being validated, the type of validation, and the settings of the model being validated\n",
    "\n",
    "#Configuration and variables for model prediction script\n",
    "# (CURIAL-Rapide, CURIAL-Lab & CURIAL-1.0 respectively) #\n",
    "featureSets = ['OLO & Vitals', 'Blood & Vitals', 'Blood & Blood_Gas & Vitals'];\n",
    "\n",
    "#Define which CURIAL sensitivies are being assessed\n",
    "recalls = ['0.800','0.900']\n",
    "\n",
    "#Define imputation method is being used during the evaluation (mode, median, mean)\n",
    "imputationMethods = ['median']\n",
    "\n",
    "#Define whether evaluating model for all-comers to hospital (ED, acute medicine, etc) or just patients who went on to be admitted\n",
    "Admission = True\n",
    "\n",
    "#Define if performing analysis just on patients with LFDs (allows comparison against LFDs)\n",
    "LFD = False\n",
    "\n",
    "#Composite pathway: if wanting to assess the combined pathway of calling COVID-19-suspected if either LFDs + CURIAL results positive, set flag to True. \n",
    "#For a standard validation, set to false\n",
    "CURIALLFDComposite = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set date range (e.g. for OUH, during second wave from 1st Oct 2020 onwards)\n",
    "dataset = dataset[dataset.ArrivalDateTime >= '2020-10-01']\n",
    "\n",
    "#Print out size\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude patients not tested for COVID-19\n",
    "dataset = dataset[~dataset['Covid-19 Positive'].isna()]\n",
    "\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covid test counts\n",
    "dataset['Covid-19 Positive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to calculate 95% CIs with Wilson's Method\n",
    "# Source: https://gist.github.com/maidens/29939b3383a5e57935491303cf0d8e0b\n",
    "# #    \n",
    "#     References\n",
    "#     ----------\n",
    "#     [1] R. G. Newcombe and D. G. Altman, Proportions and their differences, in Statisics\n",
    "#     with Confidence: Confidence intervals and statisctical guidelines, 2nd Ed., D. G. Altman, \n",
    "#     D. Machin, T. N. Bryant and M. J. Gardner (Eds.), pp. 45-57, BMJ Books, 2000. \n",
    "# #\n",
    "def _proportion_confidence_interval(r, n, z):\n",
    "    \"\"\"Compute confidence interval for a proportion.\n",
    "    \n",
    "    Follows notation described on pages 46--47 of [1]. \n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    [1] R. G. Newcombe and D. G. Altman, Proportions and their differences, in Statisics\n",
    "    with Confidence: Confidence intervals and statisctical guidelines, 2nd Ed., D. G. Altman, \n",
    "    D. Machin, T. N. Bryant and M. J. Gardner (Eds.), pp. 45-57, BMJ Books, 2000. \n",
    "    \"\"\"\n",
    "    \n",
    "    A = 2*r + z**2\n",
    "    B = z*sqrt(z**2 + 4*r*(1 - r/n))\n",
    "    C = 2*(n + z**2)\n",
    "    return ((A-B)/C, (A+B)/C)\n",
    "\n",
    "def sensitivity_and_specificity_with_confidence_intervals(TP, FP, FN, TN, alpha=0.95):\n",
    "    \"\"\"Compute confidence intervals for sensitivity and specificity using Wilson's method. \n",
    "    \n",
    "    This method does not rely on a normal approximation and results in accurate \n",
    "    confidence intervals even for small sample sizes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    TP : int\n",
    "        Number of true positives\n",
    "    FP : int \n",
    "        Number of false positives\n",
    "    FN : int\n",
    "        Number of false negatives\n",
    "    TN : int\n",
    "        Number of true negatives\n",
    "    alpha : float, optional\n",
    "        Desired confidence. Defaults to 0.95, which yields a 95% confidence interval. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sensitivity_point_estimate : float\n",
    "        Numerical estimate of the test sensitivity\n",
    "    specificity_point_estimate : float\n",
    "        Numerical estimate of the test specificity\n",
    "    sensitivity_confidence_interval : Tuple (float, float)\n",
    "        Lower and upper bounds on the alpha confidence interval for sensitivity\n",
    "    specificity_confidence_interval\n",
    "        Lower and upper bounds on the alpha confidence interval for specificity \n",
    "        \n",
    "    References\n",
    "    ----------\n",
    "    [1] R. G. Newcombe and D. G. Altman, Proportions and their differences, in Statisics\n",
    "    with Confidence: Confidence intervals and statisctical guidelines, 2nd Ed., D. G. Altman, \n",
    "    D. Machin, T. N. Bryant and M. J. Gardner (Eds.), pp. 45-57, BMJ Books, 2000. \n",
    "    [2] E. B. Wilson, Probable inference, the law of succession, and statistical inference,\n",
    "    J Am Stat Assoc 22:209-12, 1927. \n",
    "    \"\"\"\n",
    "    \n",
    "    # \n",
    "    z = -ndtri((1.0-alpha)/2)\n",
    "    \n",
    "    # Compute sensitivity using method described in [1]\n",
    "    sensitivity_point_estimate = TP/(TP + FN)\n",
    "    sensitivity_confidence_interval = _proportion_confidence_interval(TP, TP + FN, z)\n",
    "    \n",
    "    # Compute specificity using method described in [1]\n",
    "    specificity_point_estimate = TN/(TN + FP)\n",
    "    specificity_confidence_interval = _proportion_confidence_interval(TN, TN + FP, z)\n",
    "    \n",
    "    return sensitivity_point_estimate, specificity_point_estimate, sensitivity_confidence_interval, specificity_confidence_interval\n",
    "\n",
    "#### Function to calculate AUROC with 95% CIs\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov  6 10:06:52 2018\n",
    "\n",
    "@author: yandexdataschool\n",
    "\n",
    "Original Code found in:\n",
    "https://github.com/yandexdataschool/roc_comparison\n",
    "\n",
    "updated: Raul Sanchez-Vazquez\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy import stats\n",
    "\n",
    "# AUC comparison adapted from\n",
    "# https://github.com/Netflix/vmaf/\n",
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    if sample_weight is None:\n",
    "        return fastDeLong_no_weights(predictions_sorted_transposed, label_1_count)\n",
    "    else:\n",
    "        return fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight)\n",
    "\n",
    "\n",
    "def fastDeLong_weights(predictions_sorted_transposed, label_1_count, sample_weight):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank_weight(positive_examples[r, :], sample_weight[:m])\n",
    "        ty[r, :] = compute_midrank_weight(negative_examples[r, :], sample_weight[m:])\n",
    "        tz[r, :] = compute_midrank_weight(predictions_sorted_transposed[r, :], sample_weight)\n",
    "    total_positive_weights = sample_weight[:m].sum()\n",
    "    total_negative_weights = sample_weight[m:].sum()\n",
    "    pair_weights = np.dot(sample_weight[:m, np.newaxis], sample_weight[np.newaxis, m:])\n",
    "    total_pair_weights = pair_weights.sum()\n",
    "    aucs = (sample_weight[:m]*(tz[:, :m] - tx)).sum(axis=1) / total_pair_weights\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / total_negative_weights\n",
    "    v10 = 1. - (tz[:, m:] - ty[:, :]) / total_positive_weights\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def fastDeLong_no_weights(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating\n",
    "              Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count, ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.95\n",
    "\n",
    "def calculateMetricsWithProbs (preds, probs, ground_truth, alpha = alpha):\n",
    "        precision = precision_score(ground_truth,preds,zero_division=0)\n",
    "        recallAchieved = recall_score(ground_truth,preds,zero_division=0)\n",
    "        accuracy = accuracy_score(ground_truth,preds)\n",
    "        #auc = roc_auc_score(ground_truth,pred_probs) \n",
    "        #ns_fpr, ns_tpr, _ = roc_curve(ground_truth, pred_probs)\n",
    "        tn, fp, fn, tp = confusion_matrix(ground_truth,preds).ravel()\n",
    "        specificity = tn/(tn+fp)\n",
    "        npv = tn/(fn+tn)\n",
    "        \n",
    "        #Work out AUROC using delong method (in function above)\n",
    "        auc, auc_cov = delong_roc_variance(ground_truth,probs)\n",
    "        auc_std = np.sqrt(auc_cov)\n",
    "        lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "        auc_ci = np.around(stats.norm.ppf(lower_upper_q,loc=auc,scale=auc_std), decimals=3)\n",
    "        \n",
    "        #CIs for accuracy, PPV and NPV, Se, Sp\n",
    "        alpha = 0.95\n",
    "        z = -ndtri((1.0-alpha)/2)\n",
    "        accuracy_ci = np.around(np.around(_proportion_confidence_interval(tp+tn, tp+tn+fp+fn, z), decimals=3)*100,decimals=1)\n",
    "        ppv_ci = np.around(np.around(_proportion_confidence_interval(tp, tp+fp, z), decimals=3)*100,decimals=1)\n",
    "        npv_ci = np.around(np.around(_proportion_confidence_interval(tn, tn+fn, z), decimals=3)*100,decimals=1)\n",
    "        se_ci = np.around(np.around(_proportion_confidence_interval(tp, tp+fn, z), decimals=3)*100,decimals=1)\n",
    "        sp_ci = np.around(np.around(_proportion_confidence_interval(tn, tn+fp, z), decimals=3)*100,decimals=1)\n",
    "        \n",
    "        #Generate strings\n",
    "        accuracy_withCI = \"%s%% (%s - %s)\" % (np.round(accuracy*100, 1), accuracy_ci[0], accuracy_ci[1])\n",
    "        ppv_withCI =  \"%s%% (%s - %s)\" % (np.round(precision*100, 1), ppv_ci[0], ppv_ci[1])\n",
    "        npv_withCI =  \"%s%% (%s - %s)\" % (np.round(npv*100, 1), npv_ci[0], npv_ci[1])\n",
    "        se_withCI =  \"%s%% (%s - %s)\" % (np.round(recallAchieved*100, 1), se_ci[0], se_ci[1])\n",
    "        sp_withCI =  \"%s%% (%s - %s)\" % (np.round(specificity*100, 1), sp_ci[0], sp_ci[1])\n",
    "        roc_withCI =  \"%s (%s - %s)\" % (np.round(auc, 3), auc_ci[0], auc_ci[1])\n",
    "        \n",
    "        #Generate ROC curves\n",
    "        fpr, tpr, thresholds = roc_curve(ground_truth, probs)\n",
    "        \n",
    "        f1score = 2*(precision*recallAchieved)/(precision+recallAchieved)\n",
    "        \n",
    "        return se_withCI,sp_withCI,accuracy_withCI,roc_withCI,ppv_withCI,npv_withCI,fn,fp,f1score, fpr, tpr\n",
    "\n",
    "def calculateMetricsWithoutProbs (preds, ground_truth, alpha = alpha):\n",
    "    precision = precision_score(ground_truth,preds,zero_division=0)\n",
    "    recallAchieved = recall_score(ground_truth,preds,zero_division=0)\n",
    "    accuracy = accuracy_score(ground_truth,preds)\n",
    "    #auc = roc_auc_score(ground_truth,pred_probs) \n",
    "    #ns_fpr, ns_tpr, _ = roc_curve(ground_truth, pred_probs)\n",
    "    tn, fp, fn, tp = confusion_matrix(ground_truth,preds).ravel()\n",
    "    specificity = tn/(tn+fp)\n",
    "    npv = tn/(fn+tn)\n",
    "    f1score = 2*(precision*recallAchieved)/(precision+recallAchieved)\n",
    "\n",
    "    #CIs for accuracy, PPV and NPV, Se, Sp\n",
    "    alpha = 0.95\n",
    "    z = -ndtri((1.0-alpha)/2)\n",
    "    accuracy_ci = np.around(np.around(_proportion_confidence_interval(tp+tn, tp+tn+fp+fn, z), decimals=3)*100,decimals=1)\n",
    "    ppv_ci = np.around(np.around(_proportion_confidence_interval(tp, tp+fp, z), decimals=3)*100,decimals=1)\n",
    "    npv_ci = np.around(np.around(_proportion_confidence_interval(tn, tn+fn, z), decimals=3)*100,decimals=1)\n",
    "    se_ci = np.around(np.around(_proportion_confidence_interval(tp, tp+fn, z), decimals=3)*100,decimals=1)\n",
    "    sp_ci = np.around(np.around(_proportion_confidence_interval(tn, tn+fp, z), decimals=3)*100,decimals=1)\n",
    "    \n",
    "    #Generate strings\n",
    "    accuracy_withCI = \"%s%% (%s - %s)\" % (np.round(accuracy*100, 1), accuracy_ci[0], accuracy_ci[1])\n",
    "    ppv_withCI =  \"%s%% (%s - %s)\" % (np.round(precision*100, 1), ppv_ci[0], ppv_ci[1])\n",
    "    npv_withCI =  \"%s%% (%s - %s)\" % (np.round(npv*100, 1), npv_ci[0], npv_ci[1])\n",
    "    se_withCI =  \"%s%% (%s - %s)\" % (np.round(recallAchieved*100, 1), se_ci[0], se_ci[1])\n",
    "    sp_withCI =  \"%s%% (%s - %s)\" % (np.round(specificity*100, 1), sp_ci[0], sp_ci[1])\n",
    "    \n",
    "    return se_withCI,sp_withCI,accuracy_withCI,np.NaN,ppv_withCI,npv_withCI,fn,fp,f1score\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions, sample_weight=None):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    order, label_1_count, ordered_sample_weight = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count, ordered_sample_weight)\n",
    "    assert len(aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Results output dataframe\n",
    "results = pd.DataFrame(columns=['Feature Set','CURIAL Targetted Recall','Imputation','CURIAL Recall during training CV','Achieved Sensitivity','Specificity','Accuracy','AUROC','Precision','NPV','n False Negative','n False Positive','F1'])\n",
    "i=1;\n",
    "\n",
    "ax=pyplot.axes()\n",
    "\n",
    "for featureSet in featureSets:\n",
    "    print (\"***Feature Set: \"+featureSet)\n",
    "        #results= {}\n",
    "    \n",
    "    for recall in recalls:\n",
    "        for imputationMethod in imputationMethods:\n",
    "            #Scalar File\n",
    "            scalarFile = '../norm_'+featureSet+'.pkl'\n",
    "            modelFile = '../Results/'+featureSet+'/Recall_'+recall+'/fitted_models_dict.pkl'\n",
    "            imputerFile =  '../imputer_'+featureSet+'_'+imputationMethod+'.pkl'\n",
    "            featureListFile = '../featurelist_'+featureSet+'.pkl'\n",
    "            \n",
    "            #Restricting to past Sept 2020\n",
    "            dataset=dataset[dataset.ArrivalDateTime > '2020-09-01' ]\n",
    "            \n",
    "            if Admission:\n",
    "                dataset = dataset[dataset.Admission == 1]\n",
    "            \n",
    "            #dataset=dataset[dataset.ArrivalDateTime > '2020-12-22' ]\n",
    "\n",
    "            #Import scalar\n",
    "            scaler = pd.read_pickle(scalarFile)\n",
    "\n",
    "            #Import imputer\n",
    "            imputer = pd.read_pickle(imputerFile)\n",
    "\n",
    "            #Import models\n",
    "            models_dict = pd.read_pickle(modelFile)\n",
    "\n",
    "            #Import feature list\n",
    "            featureList = pd.read_pickle(featureListFile)\n",
    "            #print (featureList)\n",
    "\n",
    "            #Import performance file\n",
    "            performanceFile = pd.read_csv('../Results/'+featureSet+'/Recall_'+recall+'/10CV_Mean_Multiple_Imputation.csv')\n",
    "            thresholdForSensitivity = performanceFile['Threshold'][0]\n",
    "            \n",
    "            if LFD == True:\n",
    "            #Filter out only presentations with LFDs if LFD is true\n",
    "                presentationsWithLFDs = dataset[(dataset.Lateral_flow_result=='Positive') | (dataset.Lateral_flow_result=='Negative')]\n",
    "            else:\n",
    "                presentationsWithLFDs = dataset\n",
    "\n",
    "            #Select out features, in the model's required order\n",
    "            predictionSet = presentationsWithLFDs[featureList]\n",
    "\n",
    "            #Impute missing data using the correct imputer\n",
    "            imputedPredictionSet = imputer.transform(predictionSet.values)\n",
    "\n",
    "            #Apply scalar transform\n",
    "            scaledPredictionSet = scaler.transform(imputedPredictionSet)\n",
    "\n",
    "            #Select key for model with the correct Feature Set and with each Imputation\n",
    "            selectedModel = \"['covid_vs_all', True, 20, '\"+imputationMethod+\"', '\"+featureSet+\"', 'XGB', False, None]\"\n",
    "\n",
    "            #Make predictions\n",
    "            pred_probs = models_dict[selectedModel].predict_proba(pd.DataFrame(scaledPredictionSet))[:,1]\n",
    "            preds = np.where(pred_probs>thresholdForSensitivity,1,0)\n",
    "\n",
    "            #Configure Output Df\n",
    "            outputDf = pd.DataFrame(presentationsWithLFDs[['ClusterID','EpisodeID','ArrivalDateTime','Covid-19 Positive','Lateral_flow_result']])\n",
    "            lfd_ids = {'Negative': 0, 'Positive': 1}\n",
    "\n",
    "            def featurize_protected(df, protected, id_map):\n",
    "                col = df[protected].copy()\n",
    "                col.replace(id_map, inplace=True)\n",
    "                return col\n",
    "            \n",
    "            lfd_col = featurize_protected(outputDf, 'Lateral_flow_result', lfd_ids)\n",
    "            outputDf = outputDf.fillna({'Covid-19 Positive': 0})\n",
    "            outputDf['LFD_binary_result'] = lfd_col\n",
    "\n",
    "            #Add in Model Predictions\n",
    "            outputDf['CURIAL_Preds'] = pred_probs\n",
    "\n",
    "            #Apply Threshold\n",
    "            outputDf['CURIAL_Output'] = preds\n",
    "            \n",
    "            if CURIALLFDComposite:\n",
    "                #Generate LFD-Curial Composite Score\n",
    "                outputDf['LFD-Curial-Composite'] = (outputDf['CURIAL_Output'] + outputDf['LFD_binary_result']).clip(upper=1)\n",
    "                outputDf['LFD-Curial-Composite_prob'] = (outputDf['CURIAL_Preds'] + outputDf['LFD_binary_result']).clip(upper=1)\n",
    "            \n",
    "    \n",
    "            #Print out number positive and number negative\n",
    "            #outputDf['CURIAL_Output'].value_counts()\n",
    "\n",
    "            # Sensitivity during CV Val\n",
    "            crossValidationSensitivity = performanceFile['Recall'][0]\n",
    "\n",
    "            #Calculate summary metrics for CURIAL Model\n",
    "            recallAchieved,specificity,accuracy,auc,precision,npv,fn,fp,f1score, fpr, tpr = calculateMetricsWithProbs (preds, pred_probs, outputDf['Covid-19 Positive'])\n",
    "            #Write results\n",
    "            metrics = [featureSet,recall,imputationMethod,crossValidationSensitivity,recallAchieved,specificity,accuracy,auc,precision,npv,fn,fp,f1score]\n",
    "            results.loc[i,:] = metrics\n",
    "            i=i+1\n",
    "\n",
    "            #------------\n",
    "            #Now calculate summary metrics for CURIAL-LFD Composite\n",
    "            if CURIALLFDComposite:\n",
    "                #recallAchieved,specificity,accuracy,auc,precision,npv,fn,fp,f1score = calculateMetricsWithoutProbs (outputDf['LFD-Curial-Composite'], outputDf['Covid-19 Positive'])\n",
    "                recallAchieved,specificity,accuracy,auc,precision,npv,fn,fp,f1score, fpr, tpr = calculateMetricsWithProbs (outputDf['LFD-Curial-Composite'], outputDf['LFD-Curial-Composite_prob'], outputDf['Covid-19 Positive'])\n",
    "                metrics = ['Composite LFD-CURIAL: '+featureSet,recall, imputationMethod,np.NaN,recallAchieved,specificity,accuracy,auc,precision,npv,fn,fp,f1score]\n",
    "                results.loc[i,:] = metrics\n",
    "                i=i+1\n",
    "                                \n",
    "            if CURIALLFDComposite:\n",
    "                ax.plot(fpr, tpr, marker='.', label=featureSet)\n",
    "            else:\n",
    "                ax.plot(fpr, tpr, marker='.', label='Composite LFD-CURIAL: '+featureSet)\n",
    "\n",
    "            #print (featureSet+\" performance at Se: \"+recall+' VS LFD as Gold Standard')\n",
    "            #print(classification_report(outputDf['LFD_binary_result'], outputDf['CURIAL_Output']))\n",
    "\n",
    "if LFD:\n",
    "    print (\"**LFD Performance\")\n",
    "    print(classification_report(outputDf['Covid-19 Positive'], outputDf['LFD_binary_result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.xlabel('False Positive Rate')\n",
    "pyplot.ylabel('True Positive Rate')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a calibration curve\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.metrics import roc_curve\n",
    "from matplotlib import pyplot\n",
    "\n",
    "outputDf=outputDf\n",
    "\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(outputDf['Covid-19 Positive'], outputDf['CURIAL_Preds'], n_bins=15, normalize=True)\n",
    "# plot perfectly calibrated\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "pyplot.plot(mpv, fop, marker='.')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add raw LFD performance on this cohort\n",
    "if LFD:\n",
    "    recallAchieved,specificity,accuracy,auc,precision,npv,fn,fp,f1score = calculateMetricsWithoutProbs (outputDf['LFD_binary_result'], outputDf['Covid-19 Positive'])\n",
    "    metrics = ['Lateral Flow Tests',np.NaN,np.NaN,np.NaN,recallAchieved,specificity,accuracy,np.NaN,precision,npv,fn,fp,f1score]\n",
    "    results.loc[0,:] = metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out evaluation results\n",
    "results.drop(columns=['Imputation','CURIAL Recall during training CV'], inplace=True)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
